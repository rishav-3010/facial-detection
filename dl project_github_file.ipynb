{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8885852b-b4a5-4bde-82df-31111e350de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c1a3389-c35b-471d-9339-49446c9e1190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing of dataset\n",
    "\n",
    "def get_base_model():\n",
    "    base = MobileNetV2(weights='imagenet', include_top=False, input_tensor=Input(shape=(256, 256, 3)))\n",
    "    x = GlobalAveragePooling2D()(base.output)\n",
    "    model = Model(inputs=base.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "def preprocess_face(frame, face_cascade, size=(256, 256)):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    processed_faces = []\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        margin = int(0.2 * w)\n",
    "        x1 = max(0, x - margin)\n",
    "        y1 = max(0, y - margin)\n",
    "        x2 = min(frame.shape[1], x + w + margin)\n",
    "        y2 = min(frame.shape[0], y + h + margin)\n",
    "        face_img = frame[y1:y2, x1:x2]\n",
    "        face_resized = cv2.resize(face_img, size)\n",
    "        processed_faces.append((face_resized, (x1, y1, x2, y2)))\n",
    "    return processed_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f45eb20-1f1d-4b02-9a30-1acb89ff9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capturing data through webcam for creating dataset for each individual\n",
    "\n",
    "def collect_data(data_dir=\"dataset\", label=\"person\"):\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    os.makedirs(os.path.join(data_dir, label), exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    count = 0\n",
    "    print(\"[INFO] Press 'q' to stop capturing...\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        faces = preprocess_face(frame, face_cascade)\n",
    "        for face_img, (x1, y1, x2, y2) in faces:\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            filename = os.path.join(data_dir, label, f\"{count}.jpg\")\n",
    "            cv2.imwrite(filename, face_img)\n",
    "            count += 1\n",
    "\n",
    "        cv2.imshow(\"Collecting Faces\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"[INFO] Collected {count} images for '{label}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a2837c-9c79-4294-bc1c-c77618317adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model for training\n",
    "\n",
    "def create_model(num_classes):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=Input(shape=(256, 256, 3)))\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    out = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=out)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model.compile(optimizer=Adam(0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33472c01-337c-42dd-9ce5-38e3140b26a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model with data augmentation\n",
    "\n",
    "def train_model(data_dir=\"dataset\", model_save_path=\"face_model.h5\"):\n",
    "    datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input,\n",
    "        validation_split=0.2,\n",
    "        rotation_range=15,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "    )\n",
    "\n",
    "    train_gen = datagen.flow_from_directory(\n",
    "        data_dir, target_size=(256, 256), batch_size=16, subset=\"training\"\n",
    "    )\n",
    "    val_gen = datagen.flow_from_directory(\n",
    "        data_dir, target_size=(256, 256), batch_size=16, subset=\"validation\"\n",
    "    )\n",
    "\n",
    "    model, base_model = create_model(num_classes=train_gen.num_classes)\n",
    "\n",
    "    history = model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "    model.save(model_save_path)\n",
    "    print(\"[INFO] Model saved as\", model_save_path)\n",
    "    return model, base_model, history, val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b93b997-3b23-419e-8579-cc3db51dd24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    plt.title('Accuracy vs Epochs', fontsize=14)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    plt.title('Loss vs Epochs', fontsize=14)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    ######\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff9e5e60-5d69-48fc-a7de-aa2288976221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation of embeddings\n",
    "\n",
    "def generate_embeddings(base_model, data_dir=\"dataset\", save_path=\"embeddings.pkl\"):\n",
    "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    gen = datagen.flow_from_directory(data_dir, target_size=(256, 256), class_mode='sparse', shuffle=False)\n",
    "\n",
    "    embeddings = base_model.predict(gen)\n",
    "    if len(embeddings.shape) > 2:\n",
    "        embeddings = embeddings.reshape(embeddings.shape[0], -1)\n",
    "    labels = gen.classes\n",
    "    label_map = {v: k for k, v in gen.class_indices.items()}\n",
    "\n",
    "    emb_dict = {\"embeddings\": embeddings, \"labels\": labels, \"label_map\": label_map}\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(emb_dict, f)\n",
    "\n",
    "    print(\"[INFO] Saved embeddings to\", save_path)\n",
    "    return emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "463ad153-136b-4c73-bd40-4d3d9c325f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph for PCA (Principle Component Analysis) of dataset\n",
    "\n",
    "def plot_embeddings(emb_dict):\n",
    "    X = emb_dict[\"embeddings\"]\n",
    "    y = emb_dict[\"labels\"]\n",
    "    label_map = emb_dict[\"label_map\"]\n",
    "\n",
    "    if len(X.shape) > 2:\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(X)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for label in np.unique(y):\n",
    "         plt.scatter(reduced[y == label, 0], reduced[y == label, 1],\n",
    "                    label=label_map[label], alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.title(\"PCA of Face Embeddings\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1c60e9b-fea2-49f5-bdf6-259a38847da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output screen, detecting faces\n",
    "\n",
    "def recognize_faces(base_model, emb_dict, threshold=0.5):\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    embeddings = emb_dict['embeddings']\n",
    "    labels = emb_dict['labels']\n",
    "    label_map = emb_dict['label_map']\n",
    "\n",
    "    # Flatten stored embeddings before fitting NearestNeighbors\n",
    "    if len(embeddings.shape) > 2:\n",
    "        embeddings = embeddings.reshape(embeddings.shape[0], -1)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, metric='cosine').fit(embeddings)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    print(\"[INFO] Starting real-time recognition... Press 'q' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        faces = preprocess_face(frame, face_cascade)\n",
    "        for face_img, (x1, y1, x2, y2) in faces:\n",
    "            img_array = np.expand_dims(preprocess_input(face_img), axis=0)\n",
    "            emb = base_model.predict(img_array)\n",
    "\n",
    "            if len(emb.shape) > 2:   # flatten embedding to match stored dimension\n",
    "                emb = emb.reshape(emb.shape[0], -1)\n",
    "\n",
    "            if emb.shape[1] != embeddings.shape[1]:    # resize if needed\n",
    "                emb = np.resize(emb, (emb.shape[0], embeddings.shape[1]))\n",
    "\n",
    "            dist, idx = nbrs.kneighbors(emb)\n",
    "            label = label_map[labels[idx[0][0]]]\n",
    "\n",
    "            if dist[0][0] < threshold:\n",
    "                color, text = (0, 255, 0), f\"Valid ({label})\"\n",
    "            else:\n",
    "                color, text = (0, 0, 255), \"Unknown\"\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, text, (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "        cv2.imshow('Recognition', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03fece03-e0a7-4523-92ee-dfd90dcf26e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN function to run program\n",
    "\n",
    "def main():\n",
    "    print(\"\\n========== Facial Recognition Pipeline ==========\")\n",
    "    print(\"1. Collect dataset from webcam\")\n",
    "    print(\"2. Train model with data augmentation\")\n",
    "    print(\"3. Generate & visualize embeddings\")\n",
    "    print(\"4. Run real-time recognition\")\n",
    "    print(\"=================================================\\n\")\n",
    "\n",
    "    choice = input(\"Select step to start from (1-4): \")\n",
    "\n",
    "    if choice == \"1\":\n",
    "        label = input(\"Enter label/name for person: \")\n",
    "        collect_data(data_dir=\"dataset\", label=label)\n",
    "        proceed = input(\"Proceed to training? (y/n): \")\n",
    "        if proceed.lower() != \"y\":\n",
    "            return\n",
    "\n",
    "    # Initialize emb_dict as None\n",
    "    emb_dict = None\n",
    "\n",
    "    if choice in [\"1\", \"2\"]:\n",
    "        model, base_model, history, val_gen = train_model(\"dataset\")\n",
    "        # Generate and store embeddings\n",
    "        emb_dict = generate_embeddings(base_model, \"dataset\")\n",
    "    else:\n",
    "        # Load pre-trained model if exists\n",
    "        try:\n",
    "            from tensorflow.keras.models import load_model\n",
    "            print(\"[INFO] Loading existing model...\")\n",
    "            model = load_model(\"face_model.h5\", compile=False)\n",
    "            model.compile(optimizer=Adam(0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            base_model = Model(inputs=model.input, outputs=model.layers[-3].output)\n",
    "            print(\"[INFO] Model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] Could not load model:\", e)\n",
    "            print(\"[INFO] Please run option 2 to train the model first\")\n",
    "            return\n",
    "\n",
    "    # Handle embeddings\n",
    "    try:\n",
    "        if choice not in [\"1\", \"2\"]:  # Only load if we didn't just generate them\n",
    "            import pickle\n",
    "            print(\"[INFO] Loading saved embeddings...\")\n",
    "            with open(\"embeddings.pkl\", \"rb\") as f:\n",
    "                emb_dict = pickle.load(f)\n",
    "            print(\"[INFO] Embeddings loaded successfully\")\n",
    "\n",
    "            # Verify model matches dataset\n",
    "            datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "            gen = datagen.flow_from_directory(\"dataset\", target_size=(256, 256), \n",
    "                                            class_mode='sparse', shuffle=False, batch_size=1)\n",
    "            if len(gen.class_indices) != model.layers[-1].units:\n",
    "                print(\"[ERROR] Model classes don't match dataset. Please retrain with option 2\")\n",
    "                return\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Could not load embeddings:\", e)\n",
    "        print(\"[INFO] Please run option 2 to regenerate embeddings\")\n",
    "        return\n",
    "\n",
    "    # Only plot embeddings for options 1-3 if we have embeddings\n",
    "    if choice in [\"1\", \"2\", \"3\"] and emb_dict is not None:\n",
    "        plot_embeddings(emb_dict)\n",
    "\n",
    "    print(\"\\n[INFO] Starting real-time recognition...\")\n",
    "    recognize_faces(base_model, emb_dict)\n",
    "\n",
    "    # Only show training plots if we actually trained\n",
    "    if choice in [\"1\", \"2\"] and 'history' in locals():\n",
    "        plot_training_history(history)\n",
    "        \n",
    "        print(\"[INFO] Generating confusion matrix...\")\n",
    "        val_gen.reset()\n",
    "        Y_pred = model.predict(val_gen)\n",
    "        y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "        cm = confusion_matrix(val_gen.classes, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                    display_labels=list(val_gen.class_indices.keys()))\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        disp.plot(cmap='Blues', values_format='d')\n",
    "        plt.title(\"Confusion Matrix - Validation Data\", fontsize=14)\n",
    "        plt.grid(False)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bceaa728-1439-42f0-92bc-98bb6a31968a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Facial Recognition Pipeline ==========\n",
      "1. Collect dataset from webcam\n",
      "2. Train model with data augmentation\n",
      "3. Generate & visualize embeddings\n",
      "4. Run real-time recognition\n",
      "=================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select step to start from (1-4):  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading existing model...\n",
      "[INFO] Model loaded successfully\n",
      "[INFO] Loading saved embeddings...\n",
      "[INFO] Embeddings loaded successfully\n",
      "Found 941 images belonging to 4 classes.\n",
      "\n",
      "[INFO] Starting real-time recognition...\n",
      "[INFO] Starting real-time recognition... Press 'q' to quit.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42418b4f-031f-4f52-b348-3f80de2cfc03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33460c7e-26e4-46f2-bf22-dffa75e61609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b95dae8-4a03-4345-91d3-8ba31f2c3fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
